{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EEG Alpha Power Decoding with LDA\n",
    "\n",
    "This script analyzes EEG data to classify two conditions using Linear Discriminant Analysis (LDA) on alpha band power (8-12 Hz). It processes data, trains a classifier, and evaluates decoding accuracy using cross-validation.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Data Preprocessing**\n",
    "- **Data Loading:**\n",
    "  - Loads preprocessed EEG data for multiple subjects from `.npz` files.\n",
    "  - Extracts trial data (`X`), labels (`y`), and sampling frequency (`fs`).\n",
    "\n",
    "- **Channel Selection:**\n",
    "  - Focuses on specific channels of interest: `O1`, `O2`, `P3`, `P4`, `P7`, and `P8`.\n",
    "  - Excludes subject-specific noisy channels based on a rejection list.\n",
    "\n",
    "- **Signal Processing:**\n",
    "  - Applies a bandpass filter (8-12 Hz) to isolate the alpha frequency band.\n",
    "  - Crops edge artifacts to minimize boundary effects.\n",
    "  - Computes the *log-transformed mean amplitude of the Hilbert transform* to quantify alpha power.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Decoding with LDA**\n",
    "- **Cross-Validation Setup:**\n",
    "  - Uses 4-fold cross-validation to split trials into training and test sets.\n",
    "\n",
    "- **Model Training and Testing:**\n",
    "  - Initializes an LDA classifier with Ledoit-Wolf covariance regularization for stability.\n",
    "  - Trains the LDA model on training data and evaluates performance on test data.\n",
    "  - Computes accuracy for each fold.\n",
    "\n",
    "- **Performance Metrics:**\n",
    "  - Calculates the mean decoding accuracy and standard error (SE) across folds for each subject.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Results Storage**\n",
    "- Compiles results into a structured numpy array, including:\n",
    "  - **Subject ID**\n",
    "  - **Mean Accuracy**\n",
    "  - **Standard Error**\n",
    "- Saves results to a designated directory for further analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Output**\n",
    "- **Metrics:** Provides subject-wise decoding accuracy and standard error.\n",
    "- **File Storage:** Saves results in `.npy` format under `decoding_results_dir` for easy integration with subsequent analyses.\n",
    "\n",
    "---\n",
    "\n",
    "## **Purpose**\n",
    "This pipeline evaluates the ability of alpha band features to decode left vs right attending trials.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results\\covert_lda_subset_results.npy\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy.signal import hilbert, butter, sosfilt\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.covariance import LedoitWolf\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import join\n",
    "\n",
    "wd = r'C:\\Users\\Radovan\\OneDrive\\Radboud\\Studentships\\Jordy Thielen\\root'\n",
    "os.chdir(wd)\n",
    "data_dir = join(wd, \"data\")\n",
    "experiment_dir = join(data_dir, \"experiment\")\n",
    "files_dir = join(experiment_dir, 'files')\n",
    "sourcedata_dir = join(experiment_dir, 'sourcedata')\n",
    "derivatives_dir = join(join(experiment_dir, 'derivatives'))\n",
    "\n",
    "analysis_dir = join(data_dir, \"analysis\")\n",
    "alpha_dir = join(analysis_dir, \"alpha\")\n",
    "decoding_results_dir = join(alpha_dir, \"decoding_results\") \n",
    "plots_dir = join(alpha_dir, \"plots\")\n",
    "features_dir = join(alpha_dir, \"plots\", \"features\")\n",
    "\n",
    "def bandpass_filter(data, lowcut, highcut, fs = 120, order=4):\n",
    "    sos = butter(order, [lowcut, highcut], btype='bandpass', fs=fs, output='sos')\n",
    "    return sosfilt(sos, data, axis=-1)\n",
    "def compute_average_hilbert_amplitude(data):\n",
    "    analytic = hilbert(data, axis=-1)\n",
    "    hilbert_amplitude = np.abs(analytic)\n",
    "    amplitude_mean = hilbert_amplitude.mean(axis=-1)\n",
    "    log_mean = np.log(amplitude_mean)\n",
    "    return log_mean\n",
    "\n",
    "subjects = [\n",
    "    \"VPpdia\", \"VPpdib\", \"VPpdic\", \"VPpdid\", \"VPpdie\", \"VPpdif\", \"VPpdig\", \"VPpdih\",\n",
    "    \"VPpdii\", \"VPpdij\", \"VPpdik\", \"VPpdil\", \"VPpdim\", \"VPpdin\", \"VPpdio\", \"VPpdip\",\n",
    "    \"VPpdiq\", \"VPpdir\", \"VPpdis\", \"VPpdit\", \"VPpdiu\", \"VPpdiv\", \"VPpdiw\", \"VPpdix\",\n",
    "    \"VPpdiy\", \"VPpdiz\", \"VPpdiza\", \"VPpdizb\", \"VPpdizc\"\n",
    "    ]\n",
    "\n",
    "picks_hubner = [\n",
    "    \"F7\", \"F3\", \"Fz\", \"F4\", \"F8\", \"FC1\", \"FC2\", \"FC5\", \"FC6\", \"FCz\", \"T7\", \"C3\", \n",
    "    \"Cz\", \"C4\", \"T8\", \"CP1\", \"CP2\", \"CP5\", \"CP6\", \"CPz\",\n",
    "    \"P7\", \"P3\", \"Pz\", \"P4\", \"P8\", \"Oz\", \"O1\", \"O2\"\n",
    "]\n",
    "\n",
    "subjects_channel_reject = {\n",
    "    \"VPpdib\": [\"CP2\"],\n",
    "    \"VPpdih\": [\"C3\"],\n",
    "    \"VPpdizb\": [\"Fz\"],\n",
    "    \"VPpdizc\": [\"FC2\"]\n",
    "}\n",
    "\n",
    "task = \"covert\"\n",
    "results_save_path = os.path.join(\"results\", f\"{task}_lda_subset_results.npy\")  # Save as numpy array\n",
    "\n",
    "# Ensure the results directory exists\n",
    "os.makedirs(os.path.dirname(results_save_path), exist_ok=True)\n",
    "\n",
    "# Initialize results storage\n",
    "results = []\n",
    "\n",
    "for subject in subjects:\n",
    "    file_dir = os.path.join(derivatives_dir, 'preprocessed', \"alpha\", f\"sub-{subject}\")\n",
    "    file_path = os.path.join(file_dir, f\"sub-{subject}_task-{task}_alpha.npz\")\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        continue\n",
    "    \n",
    "    picks_clean = picks_hubner.copy()\n",
    "    \n",
    "    #Adapt indexing for rejected channels\n",
    "    if subject in subjects_channel_reject:\n",
    "        # Get the channels to reject for this subject\n",
    "        channels_to_reject = subjects_channel_reject[subject]\n",
    "        # Remove all channels from picks_clean\n",
    "        for channel in channels_to_reject:\n",
    "            if channel in picks_clean:\n",
    "                picks_clean.remove(channel)\n",
    "\n",
    "    # Load the data from the NPZ file\n",
    "    npz_data = np.load(file_path)\n",
    "\n",
    "    # Extract data\n",
    "    X = npz_data['X']  # EEG data: trials x channels x samples\n",
    "    y = npz_data['y']  # Labels indicating cued side: trials\n",
    "    fs = npz_data['fs']  # Sampling frequency\n",
    "    fs = fs.flatten()[0]   # turn array to integer\n",
    "    \n",
    "    selected_channels = ['O1', 'O2', 'P3', 'P4', 'P7', 'P8']\n",
    "    selected_indices = [picks_clean.index(ch) for ch in selected_channels if ch in picks_clean]\n",
    "\n",
    "    X = X[:, selected_indices, :]  # Crop to selected channels\n",
    "    \n",
    "    # Apply bandpass filter and crop\n",
    "    X_filtered = bandpass_filter(X, 8, 12, fs=fs)\n",
    "    X_cropped = X_filtered[:, :, 120:-120]\n",
    "    \n",
    "    # Compute Hilbert transform\n",
    "    X = compute_average_hilbert_amplitude(X_cropped)\n",
    "    \n",
    "    # Cross-validation loop\n",
    "    fold_accuracies = []\n",
    "    n_folds = 4\n",
    "    n_trials = X.shape[0] // n_folds\n",
    "    folds = np.repeat(np.arange(n_folds), n_trials)\n",
    "\n",
    "    for i_fold in range(n_folds):\n",
    "        # Split data into training and test sets\n",
    "        X_trn, y_trn = X[folds != i_fold], y[folds != i_fold]\n",
    "        X_tst, y_tst = X[folds == i_fold], y[folds == i_fold]\n",
    "        \n",
    "        # Initialize LDA with LedoitWolf covariance estimator\n",
    "        lda = LDA(solver=\"lsqr\", covariance_estimator=LedoitWolf())\n",
    "\n",
    "        # Train the LDA model\n",
    "        lda.fit(X_trn, y_trn)\n",
    "\n",
    "        # Predict the labels for test data\n",
    "        y_pred = lda.predict(X_tst)\n",
    "\n",
    "        # Compute accuracy\n",
    "        accuracy = accuracy_score(y_tst, y_pred)\n",
    "        fold_accuracies.append(accuracy)\n",
    "\n",
    "    # Compute mean accuracy and standard error\n",
    "    mean_accuracy = np.round(np.mean(fold_accuracies), 2)\n",
    "    se = np.round(np.std(fold_accuracies) / np.sqrt(len(fold_accuracies)), 2)\n",
    "\n",
    "    # Store results as tuples\n",
    "    results.append((subject, mean_accuracy, se))\n",
    "\n",
    "# Convert results to a structured numpy array\n",
    "results_array = np.array(\n",
    "    results, dtype=[('subject', 'U10'), ('accuracy', 'f4'), ('standard_error', 'f4')]\n",
    ")\n",
    "if not os.path.exists(decoding_results_dir):\n",
    "        os.makedirs(decoding_results_dir)\n",
    "        \n",
    "np.save(join(decoding_results_dir, f\"{task}_lda_subset_results\"), results_array)\n",
    "print(f\"Results saved to {results_save_path}\")\n",
    "\n",
    "\n",
    "## Plotting\n",
    "\n",
    "# Extract relevant fields\n",
    "subject_accuracies = results_array['accuracy']\n",
    "subject_ses = results_array['standard_error']\n",
    "subjects = results_array['subject']\n",
    "\n",
    "# Compute overall mean accuracy and SE\n",
    "overall_mean_accuracy = np.mean(subject_accuracies)\n",
    "overall_se = np.mean(subject_ses)\n",
    "\n",
    "# Plot individual subject accuracies with SE as error bars\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(\n",
    "    x=range(1, len(subject_accuracies) + 1),  # Subject indices\n",
    "    y=subject_accuracies,  # Subject accuracies\n",
    "    yerr=subject_ses,  # Standard errors\n",
    "    fmt='o',\n",
    "    color='k',\n",
    "    ecolor='k',\n",
    "    capsize=3,\n",
    "    linestyle='None',\n",
    "    markersize=5,\n",
    "    label=''\n",
    ")\n",
    "\n",
    "# Add overall mean accuracy and SE as a separate point\n",
    "plt.errorbar(\n",
    "    x=[len(subject_accuracies) + 1],  # Place after all subjects\n",
    "    y=[overall_mean_accuracy],\n",
    "    yerr=[overall_se],\n",
    "    fmt='s',\n",
    "    color='r',\n",
    "    ecolor='r',\n",
    "    capsize=3,\n",
    "    markersize=7,\n",
    "    label=f'Overall Mean: {overall_mean_accuracy:.2f} ± {overall_se: .2f}'\n",
    ")\n",
    "\n",
    "# Customize plot\n",
    "n_channels = len(selected_channels)\n",
    "plt.title(f'Mean Decoding Accuracy per Subject ({n_channels}-Channel subset)', fontsize=14)\n",
    "plt.xticks(\n",
    "    ticks=range(1, len(subject_accuracies) + 2),  # Subject indices + \"Overall\"\n",
    "    labels=list(subjects) + ['Overall'],  # Subject labels + \"Overall\"\n",
    "    rotation=45,\n",
    "    fontsize=10\n",
    ")\n",
    "plt.yticks(np.linspace(0, 1, 11))  # Y-axis ticks from 0 to 1 in 0.1 increments\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.ylim([0.4, 1.1])  # Y-axis range\n",
    "plt.axhline(y=0.5, color='k', linestyle='-', linewidth=0.7, label='')  # Chance level\n",
    "plt.axhline(y=overall_mean_accuracy, color='r', linestyle='-', linewidth=0.7)  # Overall mean line\n",
    "\n",
    "# Add legend\n",
    "plt.legend(loc='lower right', fontsize=10)\n",
    "\n",
    "# Save or show the plot\n",
    "plt.tight_layout()\n",
    "plots_dir = join(alpha_dir, \"plots\")\n",
    "os.makedirs(os.path.dirname(plots_dir), exist_ok=True)\n",
    "plt.savefig(join(plots_dir, f\"{task}_alpha_lda_subset_results.png\"), dpi=300)\n",
    "plt.savefig(join(plots_dir, f\"{task}_alpha_lda_subset_results.svg\"), dpi=300)\n",
    "\n",
    "plt.show()\n",
    "plt.close()  \n",
    "\n",
    "# Overall results\n",
    "print(f\"Overall LDA accuracy with CSP: {overall_mean_accuracy:.2f} ± {overall_se:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
